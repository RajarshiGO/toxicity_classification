{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Toxic comment classification using LSTM model**"]},{"cell_type":"markdown","metadata":{},"source":["Import torch, numpy and other realted packages"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:31:04.266550Z","iopub.status.busy":"2022-11-03T06:31:04.266010Z","iopub.status.idle":"2022-11-03T06:31:06.341153Z","shell.execute_reply":"2022-11-03T06:31:06.339915Z","shell.execute_reply.started":"2022-11-03T06:31:04.266458Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torchtext\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import random_split\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{},"source":["We also need a tokenizer to break the text data into tokens and then vectorize the data by building a vocabulary. This can be achieved using the torchtext package."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:31:47.378717Z","iopub.status.busy":"2022-11-03T06:31:47.377449Z","iopub.status.idle":"2022-11-03T06:31:47.383921Z","shell.execute_reply":"2022-11-03T06:31:47.382450Z","shell.execute_reply.started":"2022-11-03T06:31:47.378671Z"}},"outputs":[],"source":["from torchtext.data import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator"]},{"cell_type":"markdown","metadata":{},"source":["Configure our device for training and other computations."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:33:24.113886Z","iopub.status.busy":"2022-11-03T06:33:24.113512Z","iopub.status.idle":"2022-11-03T06:33:24.177765Z","shell.execute_reply":"2022-11-03T06:33:24.176360Z","shell.execute_reply.started":"2022-11-03T06:33:24.113856Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{},"source":["Read the dataset and display the top 5 entries."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:31:53.243660Z","iopub.status.busy":"2022-11-03T06:31:53.243172Z","iopub.status.idle":"2022-11-03T06:31:54.802795Z","shell.execute_reply":"2022-11-03T06:31:54.801887Z","shell.execute_reply.started":"2022-11-03T06:31:53.243621Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"../input/d/julian3833/jigsaw-toxic-comment-classification-challenge/train.csv\")\n","data.head()"]},{"cell_type":"markdown","metadata":{},"source":["Separate the data and their labels"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:32:20.983500Z","iopub.status.busy":"2022-11-03T06:32:20.983120Z","iopub.status.idle":"2022-11-03T06:32:20.991029Z","shell.execute_reply":"2022-11-03T06:32:20.989868Z","shell.execute_reply.started":"2022-11-03T06:32:20.983469Z"},"trusted":true},"outputs":[],"source":["X = data[\"comment_text\"]\n","y = data[data.columns[2:]].values"]},{"cell_type":"markdown","metadata":{},"source":["Convert the data into iterator"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:32:06.435604Z","iopub.status.busy":"2022-11-03T06:32:06.435207Z","iopub.status.idle":"2022-11-03T06:32:06.440281Z","shell.execute_reply":"2022-11-03T06:32:06.439142Z","shell.execute_reply.started":"2022-11-03T06:32:06.435571Z"},"trusted":true},"outputs":[],"source":["X_iter = iter(X)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the tokenizer and build the vocabulary by defining a pipeline."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:32:08.947370Z","iopub.status.busy":"2022-11-03T06:32:08.946997Z","iopub.status.idle":"2022-11-03T06:32:18.169052Z","shell.execute_reply":"2022-11-03T06:32:18.168051Z","shell.execute_reply.started":"2022-11-03T06:32:08.947337Z"},"trusted":true},"outputs":[],"source":["tokenizer = get_tokenizer('basic_english')\n","def yield_tokens(data_iter):\n","    for text in data_iter:\n","        yield tokenizer(text)\n","vocab = build_vocab_from_iterator(yield_tokens(X), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"]},{"cell_type":"markdown","metadata":{},"source":["Write a custom pytorch dataset class for loading the data to our model for training and validation. Each input is first vectorized and capped to a fixed maximum length of 1000 words and if the number of words is less than 1000 then it is zero padded to form a vector of length 1000. The dataset object implements a getitem method that returns a data instance and its associated label."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:32:23.664337Z","iopub.status.busy":"2022-11-03T06:32:23.663365Z","iopub.status.idle":"2022-11-03T06:32:23.672312Z","shell.execute_reply":"2022-11-03T06:32:23.671164Z","shell.execute_reply.started":"2022-11-03T06:32:23.664292Z"},"trusted":true},"outputs":[],"source":["class comment_dataset(Dataset):\n","    def __init__(self, X, y, vocab, tokenizer, max_words = 1000):\n","        self.data = np.array(X)\n","        self.labels = np.array(y)\n","        self.max_words = max_words\n","    def __getitem__(self, index=0):\n","        text_data = self.data[index]\n","        labels = self.labels[index, :]\n","        text_data = vocab(tokenizer(text_data))\n","        text_data = text_data +([0]* (self.max_words-len(text_data))) if len(text_data)<self.max_words else text_data[:self.max_words]\n","        return torch.tensor(text_data, dtype = torch.int32), torch.tensor(labels, dtype = torch.float32)\n","\n","    def __len__(self):\n","        return self.data.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the dataset object defined above and also split the data into a train and a validation set using random_split and then define a train dataloader that will make batched from the dataset and feed it to the model during training."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:34:33.332557Z","iopub.status.busy":"2022-11-03T06:34:33.332166Z","iopub.status.idle":"2022-11-03T06:34:33.357842Z","shell.execute_reply":"2022-11-03T06:34:33.356878Z","shell.execute_reply.started":"2022-11-03T06:34:33.332525Z"},"trusted":true},"outputs":[],"source":["dataset = comment_dataset(X, y, vocab = vocab, tokenizer = tokenizer)\n","val_size = int(0.2*len(dataset))\n","train_size = len(dataset) - val_size\n","train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","batch_size = 60\n","train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size = batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T06:34:40.035376Z","iopub.status.busy":"2022-11-03T06:34:40.034974Z","iopub.status.idle":"2022-11-03T06:34:40.039828Z","shell.execute_reply":"2022-11-03T06:34:40.038851Z","shell.execute_reply.started":"2022-11-03T06:34:40.035330Z"},"trusted":true},"outputs":[],"source":["vocab_size = len(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["Next, write the model definition. It consists of an embedding layer, followed by an LSTM layer with 128 units and then finally a dense layer as output. The model outputs a vector of length 6 corresponding to each of the 6 classes."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T09:26:19.288708Z","iopub.status.busy":"2022-11-03T09:26:19.288330Z","iopub.status.idle":"2022-11-03T09:26:19.298402Z","shell.execute_reply":"2022-11-03T09:26:19.297363Z","shell.execute_reply.started":"2022-11-03T09:26:19.288675Z"},"trusted":true},"outputs":[],"source":["embed_len = 50\n","hidden_dim = 128\n","n_layers=2\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self):\n","        super(LSTMClassifier, self).__init__()\n","        self.seq = nn.Sequential(nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_len))\n","        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_len)\n","        self.lstm = nn.LSTM(input_size = embed_len, hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, bidirectional = True)\n","        self.linear = nn.Sequential(nn.Linear(2 * hidden_dim, 128),\n","                                    nn.ReLU(),\n","                                    nn.Linear(128, 6),\n","                                    nn.Sigmoid())\n","        \n","\n","    def forward(self, X_batch):\n","        embeddings = self.embedding_layer(X_batch)\n","        output, hidden = self.lstm(embeddings, (torch.randn(2 * n_layers, len(X_batch), hidden_dim, device = device), torch.randn(2 * n_layers, len(X_batch), hidden_dim, device = device)))\n","        return self.linear(output[:,-1])\n"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the model object"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T09:26:23.572572Z","iopub.status.busy":"2022-11-03T09:26:23.572177Z","iopub.status.idle":"2022-11-03T09:26:23.807647Z","shell.execute_reply":"2022-11-03T09:26:23.806689Z","shell.execute_reply.started":"2022-11-03T09:26:23.572537Z"},"trusted":true},"outputs":[],"source":["model = LSTMClassifier()\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize binary crossentropy loss since the task at hand is a multi-label classification problem where we may get multiple class outputs. We also use Adam optimizer with a learning rate of 0.01"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T09:26:27.084755Z","iopub.status.busy":"2022-11-03T09:26:27.084355Z","iopub.status.idle":"2022-11-03T09:26:27.089937Z","shell.execute_reply":"2022-11-03T09:26:27.088959Z","shell.execute_reply.started":"2022-11-03T09:26:27.084723Z"},"trusted":true},"outputs":[],"source":["loss = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T09:26:28.412699Z","iopub.status.busy":"2022-11-03T09:26:28.412029Z","iopub.status.idle":"2022-11-03T09:26:28.418466Z","shell.execute_reply":"2022-11-03T09:26:28.416424Z","shell.execute_reply.started":"2022-11-03T09:26:28.412662Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["Train the model for 20 epochs"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T09:26:30.976758Z","iopub.status.busy":"2022-11-03T09:26:30.976073Z","iopub.status.idle":"2022-11-03T11:49:58.474270Z","shell.execute_reply":"2022-11-03T11:49:58.473493Z","shell.execute_reply.started":"2022-11-03T09:26:30.976720Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:08<00:00,  4.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14374395976144178\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:09<00:00,  4.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.1415056114380871\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:09<00:00,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14170061135874654\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:09<00:00,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14133674850428296\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14120925655168362\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.1411624792341801\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14113805281292452\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.14107228560317167\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.1142057116910171\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.06085391862329299\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.050403507525728024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.04495086714505442\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.04075624751326667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:11<00:00,  4.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.03712931446133915\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.03397335843470856\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.03136015334552951\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.029051036354600972\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:09<00:00,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.026950459177869493\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.02502029775255631\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2128/2128 [07:10<00:00,  4.94it/s]"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.023210494066314227\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["epochs = 20\n","for i in range(epochs):\n","    running_loss = 0.0\n","    c = 0\n","    for data, label in tqdm(train_dataloader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        c += 1\n","        optimizer.zero_grad()\n","        pred = model(data)\n","        loss_value = loss(pred, label)\n","        running_loss += loss_value.item()\n","        loss_value.backward()\n","        optimizer.step()\n","    print(\"Loss: {}\".format(running_loss/c))"]},{"cell_type":"markdown","metadata":{},"source":["Save the trained model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T17:12:38.226945Z","iopub.status.busy":"2022-09-20T17:12:38.226053Z","iopub.status.idle":"2022-09-20T17:12:38.508100Z","shell.execute_reply":"2022-09-20T17:12:38.507119Z","shell.execute_reply.started":"2022-09-20T17:12:38.226898Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), \"model.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["Save the vocabulary"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-09-16T13:57:52.432960Z","iopub.status.busy":"2022-09-16T13:57:52.432278Z","iopub.status.idle":"2022-09-16T13:57:52.684066Z","shell.execute_reply":"2022-09-16T13:57:52.683049Z","shell.execute_reply.started":"2022-09-16T13:57:52.432923Z"},"trusted":true},"outputs":[],"source":["torch.save(vocab, \"vocab.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["Initialize validation dataloader"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T11:55:17.272015Z","iopub.status.busy":"2022-11-03T11:55:17.271315Z","iopub.status.idle":"2022-11-03T11:55:17.276618Z","shell.execute_reply":"2022-11-03T11:55:17.275659Z","shell.execute_reply.started":"2022-11-03T11:55:17.271977Z"},"trusted":true},"outputs":[],"source":["valid_loader = torch.utils.data.DataLoader(val_ds, batch_size = batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["Import accuracy metric from pytorch-iginite for model evaluation"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T12:04:21.642343Z","iopub.status.busy":"2022-11-03T12:04:21.641979Z","iopub.status.idle":"2022-11-03T12:04:21.648709Z","shell.execute_reply":"2022-11-03T12:04:21.647464Z","shell.execute_reply.started":"2022-11-03T12:04:21.642311Z"},"trusted":true},"outputs":[],"source":["from ignite.metrics import Accuracy\n","acc = Accuracy(is_multilabel=True, device = device)"]},{"cell_type":"markdown","metadata":{},"source":["Validate the model"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-11-03T12:04:25.561853Z","iopub.status.busy":"2022-11-03T12:04:25.561193Z","iopub.status.idle":"2022-11-03T12:08:03.169986Z","shell.execute_reply":"2022-11-03T12:08:03.168930Z","shell.execute_reply.started":"2022-11-03T12:04:25.561816Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.9641696107538169\n"]}],"source":["acc.reset()\n","for data, label in train_dataloader:\n","    data = data.to(\"cuda\")\n","    label = label.to(\"cuda\")\n","    with torch.no_grad():\n","        pred = model(data)\n","    pred = torch.where(pred<0.5, 0, 1)\n","    acc.update((pred, label))\n","print(\"Accuracy: \", acc.compute())"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
